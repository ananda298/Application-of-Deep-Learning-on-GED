{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vfLLhyStVvRa"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import torch\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "import torch.nn as nn\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ92djyGjK2e",
        "outputId": "c2b18ca9-07e1-47b7-ad6b-65b355077091"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install torch\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install texttable\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxxzAujUl3hc",
        "outputId": "7934ff32-e540-4eb3-e19a-930ac56869e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.7/dist-packages (1.6.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (2.1.0.post1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo | grep model\\ name\n",
        "!cat /proc/meminfo | grep MemTotal\n",
        "!/opt/bin/nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwDLdtmbcJsh",
        "outputId": "cb9a894d-a012-4109-f3b1-42e4ea955e5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "MemTotal:       13297228 kB\n",
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "eZjXYI2agZHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the test and traing data"
      ],
      "metadata": {
        "id": "qnosxO1sglIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File is loaded into a list. The list contains dictionary of each *json files. (One json files consists of p\n",
        "# pair graphs with its labels, edges and ged)\n",
        "\n",
        "path_to_train = '/content/drive/MyDrive/Capstone_Project/syn_data/train/' #change the file location to the location where the train and test file is stored\n",
        "train = []\n",
        "\n",
        "for file_name in [file for file in os.listdir(path_to_train) if file.endswith('json')]:\n",
        "    with open(path_to_train + file_name) as json_file:\n",
        "        data = json.load(json_file)\n",
        "        train.append(data)\n",
        "\n",
        "path_to_test = '/content/drive/MyDrive/Capstone_Project/syn_data/test/' #change the file location to the location where the train and test file is stored\n",
        "test = []\n",
        "\n",
        "for file_name in [file for file in os.listdir(path_to_test) if file.endswith('json')]:\n",
        "    with open(path_to_test + file_name) as json_file:\n",
        "        data = json.load(json_file)\n",
        "        test.append(data)"
      ],
      "metadata": {
        "id": "02ulUrJbgepr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the unique labels"
      ],
      "metadata": {
        "id": "2hAyAaNBgoLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is used to gather the unique labels of all the graphs\n",
        "\n",
        "train_and_test = train + test\n",
        "\n",
        "labels = [(d[\"labels_1\"], d[\"labels_2\"]) for d in train_and_test]\n",
        "\n",
        "flat_labels = []\n",
        "for a_tuple in labels:\n",
        "    flat_labels.extend(list(a_tuple))\n",
        "\n",
        "flatten_labels = list(np.concatenate(flat_labels).flat)\n",
        "\n",
        "unique_labels = set(flatten_labels)\n",
        "\n",
        "unique_labels = list(unique_labels) \n",
        "\n",
        "features = {val:index  for index, val in enumerate(unique_labels)}\n",
        "\n",
        "length_of_labels = len(unique_labels)"
      ],
      "metadata": {
        "id": "hPPQxnf-grsR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating batches"
      ],
      "metadata": {
        "id": "tm4PJ287gtUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batches_gen(batch_size, train_data):\n",
        "    batches = []\n",
        "    train = shuffle(train_data)\n",
        "    for graph in range(0, len(train), batch_size):\n",
        "        batches.append(train[graph:graph+batch_size])\n",
        "    return batches"
      ],
      "metadata": {
        "id": "ztJ6yfjqry4D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Adjacency Matrix List"
      ],
      "metadata": {
        "id": "1Al3sdxegxBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "adjacency matrix converted code inspired from https://github.com/pulkit1joshi/SimGNN"
      ],
      "metadata": {
        "id": "8cVWtlrBpakB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_matrix(data, features):\n",
        "        matrix_data = dict()\n",
        "\n",
        "        edges_1 = data[\"graph_1\"] + [[y, x] for x, y in data[\"graph_1\"]]\n",
        "        size = max(max(edges_1))+1\n",
        "        r = [[0 for i in range(size)] for j in range(size)]\n",
        "        for row,col in edges_1:\n",
        "            r[row][col] = 1\n",
        "        r=np.array(r)\n",
        "        edges_1 = r\n",
        "        edges_2 = data[\"graph_2\"] + [[y, x] for x, y in data[\"graph_2\"]]\n",
        "        size = max(max(edges_2))+1\n",
        "        r = [[0 for i in range(size)] for j in range(size)]\n",
        "        for row,col in edges_2:\n",
        "            r[row][col] = 1\n",
        "        r=np.array(r)\n",
        "        edges_2 = r\n",
        "\n",
        "        features_1, features_2 = [], []\n",
        "        for n in data[\"labels_1\"]:\n",
        "            features_1.append([1.0 if features[n] == i else 0.0 for i in features.values()])\n",
        "        for n in data[\"labels_2\"]:\n",
        "            features_2.append([1.0 if features[n] == i else 0.0 for i in features.values()])\n",
        "        features_1 = tf.convert_to_tensor(np.array(features_1), dtype=tf.float32)\n",
        "        features_2 = tf.convert_to_tensor(np.array(features_2), dtype=tf.float32)\n",
        "        matrix_data[\"edge_index_1\"] = edges_1\n",
        "        matrix_data[\"edge_index_2\"] = edges_2\n",
        "        matrix_data[\"features_1\"] = features_1\n",
        "        matrix_data[\"features_2\"] = features_2\n",
        "        norm_ged = data[\"ged\"]/(0.5*(len(data[\"labels_1\"])+len(data[\"labels_2\"])))\n",
        "\n",
        "        matrix_data[\"target\"] = tf.reshape(tf.convert_to_tensor(np.exp(-norm_ged).reshape(1, 1)),-1)\n",
        "\n",
        "        return matrix_data "
      ],
      "metadata": {
        "id": "9Mfa646zB-DW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be a list of the adjacency matrix for each pair of graphs. \n",
        "\n",
        "matrix_data = []\n",
        "range_train = len(train)\n",
        "\n",
        "for i in range(range_train):\n",
        "    converted = new_matrix(train[i], features)\n",
        "    matrix_data.append(converted)"
      ],
      "metadata": {
        "id": "xCrsDlIPAA70"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be a list of the adjacency matrix for each pair of graphs. \n",
        "\n",
        "matrix_data = []\n",
        "range_test = len(test)\n",
        "\n",
        "for i in range(range_test):\n",
        "    converted = new_matrix(test[i], features)\n",
        "    matrix_data.append(converted)"
      ],
      "metadata": {
        "id": "6BppCxemABXj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting GED"
      ],
      "metadata": {
        "id": "er_kSsFUjjsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GED of train \n",
        "train_ged = [(d[\"ged\"]) for d in train]\n",
        "\n",
        "# Get GED of test\n",
        "test_ged = [(d[\"ged\"]) for d in test]\n"
      ],
      "metadata": {
        "id": "o8mRTcvmjllS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GED"
      ],
      "metadata": {
        "id": "rznS4mo6YHck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used to gather the GED and shaped it accordingly for the model\n",
        "\n",
        "def ged(x,k):\n",
        "  if len(x.shape) == 3 and len(k.shape) == 3:\n",
        "    b = x.shape[0]\n",
        "    return np.matmul(np.matmul(x.reshape(b,1,-1), k), x.reshape(b,-1,1)).reshape(b)\n",
        "  elif len(x.shape) == 2 and len(k.shape) == 2:\n",
        "    return np.matmul(np.matmul.reshape(1,-1), x.reshape(-1,1)).reshape(1)\n",
        "  else:\n",
        "    raise ValueError('Input dimenstions not supported')\n"
      ],
      "metadata": {
        "id": "kaVorFYRYIg-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SimGNN Model"
      ],
      "metadata": {
        "id": "HsuXRsy0r3qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NTN Layer"
      ],
      "metadata": {
        "id": "WFdvsRN5mCeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TenorNetworkModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN Tensor Network module to calculate similarity vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        \"\"\"\n",
        "        super(TenorNetworkModule, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "    def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.model_config[\"filters_3\"],\n",
        "                                                             self.model_config[\"filters_3\"],\n",
        "                                                             self.model_config[\"tensor_neurons\"]))\n",
        "\n",
        "        self.weight_matrix_block = torch.nn.Parameter(torch.Tensor(self.model_config[\"tensor_neurons\"],\n",
        "                                                                   2*self.model_config[\"filters_3\"]))\n",
        "        self.bias = torch.nn.Parameter(torch.Tensor(self.model_config[\"tensor_neurons\"], 1))\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initializing weights.\n",
        "        \"\"\"\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix_block)\n",
        "        torch.nn.init.xavier_uniform_(self.bias)\n",
        "\n",
        "    def forward(self, embedding_1, embedding_2):\n",
        "        \"\"\"\n",
        "        Making a forward propagation pass to create a similarity vector.\n",
        "        :param embedding_1: Result of the 1st embedding after attention.\n",
        "        :param embedding_2: Result of the 2nd embedding after attention.\n",
        "        :return scores: A similarity score vector.\n",
        "        \"\"\"\n",
        "        scoring = torch.mm(torch.t(embedding_1), self.weight_matrix.view(self.model_config[\"filters_3\"], -1))\n",
        "        scoring = scoring.view(self.model_config[\"filters_3\"], self.model_config[\"tensor_neurons\"])\n",
        "        scoring = torch.mm(torch.t(scoring), embedding_2)\n",
        "        combined_representation = torch.cat((embedding_1, embedding_2))\n",
        "        block_scoring = torch.mm(self.weight_matrix_block, combined_representation)\n",
        "        scores = torch.nn.functional.relu(scoring + block_scoring + self.bias)\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "T7qGIgL4mDog"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "LoaibLVSsAZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AttentionModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN Attention Module to make a pass on graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        \"\"\"\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "    def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.model_config[\"filters_3\"],\n",
        "                                                             self.model_config[\"filters_3\"]))\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initializing weights.\n",
        "        \"\"\"\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \"\"\"\n",
        "        Making a forward propagation pass to create a graph level representation.\n",
        "        :param embedding: Result of the GCN.\n",
        "        :return representation: A graph level representation vector.\n",
        "        \"\"\"\n",
        "        global_context = torch.mean(torch.matmul(embedding, self.weight_matrix), dim=0)\n",
        "        transformed_global = torch.tanh(global_context)\n",
        "        sigmoid_scores = torch.sigmoid(torch.mm(embedding, transformed_global.view(-1, 1)))\n",
        "        representation = torch.mm(torch.t(embedding), sigmoid_scores)\n",
        "        return representation"
      ],
      "metadata": {
        "id": "YmGu3rJkmIyD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimGNN Layer Set UP"
      ],
      "metadata": {
        "id": "iljrNSubsEfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "class SimGNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n",
        "    https://arxiv.org/abs/1808.05689\n",
        "    \"\"\"\n",
        "    def __init__(self, config, number_of_labels):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        :param number_of_labels: Number of node labels.\n",
        "        \"\"\"\n",
        "        super(SimGNN, self).__init__()\n",
        "        self.model_config = config['model_config']\n",
        "        self.number_labels = number_of_labels\n",
        "        self.setup_layers()\n",
        "\n",
        "    def calculate_bottleneck_features(self):\n",
        "        \"\"\"\n",
        "        Deciding the shape of the bottleneck layer.\n",
        "        \"\"\"\n",
        "        if self.model_config[\"histogram\"] == True:\n",
        "            self.feature_count = self.model_config[\"tensor_neurons\"] + self.model_config[\"bins\"]\n",
        "        else:\n",
        "            self.feature_count = self.model_config[\"tensor_neurons\"]\n",
        "\n",
        "    def setup_layers(self):\n",
        "        \"\"\"\n",
        "        Creating the layers.\n",
        "        \"\"\"\n",
        "        self.calculate_bottleneck_features()\n",
        "        self.convolution_1 = GCNConv(self.number_labels, self.model_config[\"filters_1\"])\n",
        "        self.convolution_2 = GCNConv(self.model_config[\"filters_1\"], self.model_config[\"filters_2\"])\n",
        "        self.convolution_3 = GCNConv(self.model_config[\"filters_2\"], self.model_config[\"filters_3\"])\n",
        "\n",
        "        # self.convolution_4 = GCNConv(self.model_config[\"filters_3\"], self.model_config[\"filters_4\"])\n",
        "        # self.convolution_5 = GCNConv(self.model_config[\"filters_4\"], self.model_config[\"filters_5\"])\n",
        "        \n",
        "        self.attention = AttentionModule(self.model_config)\n",
        "        self.tensor_network = TenorNetworkModule(self.model_config)\n",
        "        self.fully_connected_first = torch.nn.Linear(self.feature_count,\n",
        "                                                     self.model_config[\"bottle_neck_neurons\"])\n",
        "        self.scoring_layer = torch.nn.Linear(self.model_config[\"bottle_neck_neurons\"], 1)\n",
        "\n",
        "    def calculate_histogram(self, abstract_features_1, abstract_features_2):\n",
        "        \"\"\"\n",
        "        Calculate histogram from similarity matrix.\n",
        "        :param abstract_features_1: Feature matrix for graph 1.\n",
        "        :param abstract_features_2: Feature matrix for graph 2.\n",
        "        :return hist: Histsogram of similarity scores.\n",
        "        \"\"\"\n",
        "        scores = torch.mm(abstract_features_1, abstract_features_2).detach()\n",
        "        scores = scores.view(-1, 1)\n",
        "        hist = torch.histc(scores, bins=self.model_config[\"bins\"])\n",
        "        hist = hist/torch.sum(hist)\n",
        "        hist = hist.view(1, -1)\n",
        "        return hist\n",
        "\n",
        "    def convolutional_pass(self, edge_index, features):\n",
        "        \"\"\"\n",
        "        Making convolutional pass.\n",
        "        :param edge_index: Edge indices.\n",
        "        :param features: Feature matrix.\n",
        "        :return features: Absstract feature matrix.\n",
        "        \"\"\"\n",
        "        features = self.convolution_1(features, edge_index)\n",
        "        features = torch.nn.functional.relu(features)\n",
        "        features = torch.nn.functional.dropout(features,\n",
        "                                               p=self.model_config[\"dropout\"],\n",
        "                                               training=self.training)\n",
        "\n",
        "        features = self.convolution_2(features, edge_index)\n",
        "        features = torch.nn.functional.relu(features)\n",
        "        features = torch.nn.functional.dropout(features,\n",
        "                                               p=self.model_config[\"dropout\"],\n",
        "                                               training=self.training)\n",
        "\n",
        "\n",
        "        features = self.convolution_3(features, edge_index)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Forward pass with graphs.\n",
        "        :param data: Data dictiyonary.\n",
        "        :return score: Similarity score.\n",
        "        \"\"\"\n",
        "        edge_index_1 = data[\"edge_index_1\"]\n",
        "        edge_index_2 = data[\"edge_index_2\"]\n",
        "        features_1 = data[\"features_1\"]\n",
        "        features_2 = data[\"features_2\"]\n",
        "\n",
        "        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)\n",
        "        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)\n",
        "\n",
        "        hist = self.calculate_histogram(abstract_features_1,\n",
        "                                            torch.t(abstract_features_2))\n",
        "\n",
        "        pooled_features_1 = self.attention(abstract_features_1)\n",
        "        pooled_features_2 = self.attention(abstract_features_2)\n",
        "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
        "        scores = torch.t(scores)\n",
        "\n",
        "        scores = torch.cat((scores, hist), dim=1).view(1, -1)\n",
        "\n",
        "        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n",
        "        \n",
        "        # scores = torch.sigmoid(self.fully_connected_first(scores))\n",
        "\n",
        "        # scores = torch.tanh(self.fully_connected_first(scores))\n",
        "        score = torch.sigmoid(self.scoring_layer(scores))\n",
        "        return score"
      ],
      "metadata": {
        "id": "bMAAaJn0mLS5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "Vdd1vhFSsLdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class SimGNNTrainer(object):\n",
        "    \"\"\"\n",
        "    SimGNN model trainer.\n",
        "    \"\"\"\n",
        "    def __init__(self, global_labels, config):\n",
        "        self.global_labels = global_labels\n",
        "        self.config = config\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"\n",
        "        Creating a SimGNN.\n",
        "        \"\"\"\n",
        "        self.model = SimGNN(self.config, len(self.global_labels))\n",
        "\n",
        "    def transfer_to_torch(self, data):\n",
        "        new_data = dict()\n",
        "        edges_1 = data[\"graph_1\"] + [[y, x] for x, y in data[\"graph_1\"]]\n",
        "\n",
        "        edges_2 = data[\"graph_2\"] + [[y, x] for x, y in data[\"graph_2\"]]\n",
        "\n",
        "        edges_1 = torch.from_numpy(np.array(edges_1, dtype=np.int64).T).type(torch.long)\n",
        "        edges_2 = torch.from_numpy(np.array(edges_2, dtype=np.int64).T).type(torch.long)\n",
        "\n",
        "        features_1, features_2 = [], []\n",
        "\n",
        "        for n in data[\"labels_1\"]:\n",
        "            features_1.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])\n",
        "\n",
        "        for n in data[\"labels_2\"]:\n",
        "            features_2.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])\n",
        "\n",
        "        features_1 = torch.FloatTensor(np.array(features_1))\n",
        "        features_2 = torch.FloatTensor(np.array(features_2))\n",
        "\n",
        "        new_data[\"edge_index_1\"] = edges_1\n",
        "        new_data[\"edge_index_2\"] = edges_2\n",
        "\n",
        "        new_data[\"features_1\"] = features_1\n",
        "        new_data[\"features_2\"] = features_2\n",
        "\n",
        "        norm_ged = data[\"ged\"]/(0.5*(len(data[\"labels_1\"])+len(data[\"labels_2\"])))\n",
        "\n",
        "        new_data[\"target\"] = torch.from_numpy(np.exp(-norm_ged).reshape(1, 1)).view(-1).float()\n",
        "        return new_data\n",
        "\n",
        "    def process_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Forward pass with a batch of data.\n",
        "        :param batch: Batch of graph pair locations.\n",
        "        :return loss: Loss on the batch.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        losses = 0\n",
        "        for graph_pair in batch:\n",
        "            data = graph_pair\n",
        "            data = self.transfer_to_torch(data)\n",
        "            target = data[\"target\"]\n",
        "            prediction = self.model(data)\n",
        "            losses = losses + torch.nn.functional.mse_loss(target, prediction.view(-1))\n",
        "        losses.backward(retain_graph=True)\n",
        "        self.optimizer.step()\n",
        "        loss = losses.item()\n",
        "        return loss\n",
        "\n",
        "    def fit(self):\n",
        "        print(\"\\nModel training.\\n\")\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                          lr=self.config['optim_config']['learning_rate'],\n",
        "                                          weight_decay=self.config['optim_config']['weight_decay'])\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        for epoch in range(1,self.config['optim_config']['epochs']+1):\n",
        "            batches = self.config['data_config']['batches']\n",
        "            self.loss_sum = 0\n",
        "            main_index = 0\n",
        "            batches = tqdm(enumerate(batches), total=len(batches), leave = True)\n",
        "            for index, batch in batches:\n",
        "                loss_score = self.process_batch(batch)/len(batch)\n",
        "                main_index = main_index + len(batch)\n",
        "                self.loss_sum = self.loss_sum + loss_score\n",
        "                batches.set_description(\"(Batch Loss=%g)\" % round(loss_score, 5))\n",
        "            print(\"Epoch {epoch}/{len_epoches} Loss:{loss:.5f}\".format(epoch=epoch,\n",
        "                                                                       len_epoches=self.config['optim_config']['epochs'],\n",
        "                                                                       loss=self.loss_sum/len(batches)))\n",
        "\n",
        "    def calculate_normalized_ged(self, data):\n",
        "        norm_ged = data[\"ged\"]/(0.5*(len(data[\"labels_1\"])+len(data[\"labels_2\"])))\n",
        "        return norm_ged\n",
        "\n",
        "    def calculate_loss(self,prediction, target):\n",
        "        score = torch.nn.functional.mse_loss(target, prediction)\n",
        "        return score\n",
        "\n",
        "    def score(self):\n",
        "        print(\"\\n\\nModel evaluation.\\n\")\n",
        "        self.model.eval()\n",
        "        self.scores = []\n",
        "        self.ground_truth = []\n",
        "        for graph_pair in tqdm(self.config['data_config']['test_data']):\n",
        "            data = graph_pair\n",
        "            self.ground_truth.append(self.calculate_normalized_ged(data))\n",
        "            data = self.transfer_to_torch(data)\n",
        "            target = data[\"target\"]\n",
        "            with torch.no_grad():\n",
        "                prediction = self.model(data)\n",
        "            self.scores.append(self.calculate_loss(prediction.view(-1), target))\n",
        "        self.print_evaluation()\n",
        "\n",
        "    def print_evaluation(self):\n",
        "        \"\"\"\n",
        "        Printing the error rates.\n",
        "        \"\"\"\n",
        "        norm_ged_mean = np.mean(self.ground_truth)\n",
        "        base_error = np.mean([(n-norm_ged_mean)**2 for n in self.ground_truth])\n",
        "        model_error = np.mean(self.scores)\n",
        "        print(\"\\nBaseline error: \" +str(round(base_error, 5))+\".\")\n",
        "        print(\"\\nModel test error: \" +str(round(model_error, 5))+\".\")\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(), self.config['run_config']['out_dir'])\n",
        "\n",
        "    def load(self):\n",
        "        self.model.load_state_dict(torch.load(self.config['run_config']['load_dir']))"
      ],
      "metadata": {
        "id": "pcPi_CkCmRn1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Runner"
      ],
      "metadata": {
        "id": "DWX1e4WqsPp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running config"
      ],
      "metadata": {
        "id": "xh-jDExksRqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "def config_setup():\n",
        "    model_config = OrderedDict([\n",
        "        ('histogram', True),\n",
        "        ('bins', 32),\n",
        "        ('filters_1', 128),\n",
        "        ('filters_2', 64),\n",
        "        ('filters_3', 32),\n",
        "        # ('filters_4', 32),\n",
        "        # ('filters_5', 32),\n",
        "        ('tensor_neurons', 16),\n",
        "        ('bottle_neck_neurons', 16),\n",
        "        ('dropout', 0.5)\n",
        "    ])\n",
        "\n",
        "    optim_config = OrderedDict([\n",
        "        ('epochs', 5),\n",
        "        ('batch_size', 256),\n",
        "        ('learning_rate', 0.05),\n",
        "        ('weight_decay', 0),\n",
        "        ('momentum', 0.9),\n",
        "        ('lr_decay', 0.1),\n",
        "        ('epoch_decay_begin', 50)\n",
        "    ])\n",
        "\n",
        "    run_config = OrderedDict([\n",
        "        ('seed', 1),\n",
        "        ('training_root','./dataset/train/'),\n",
        "        ('testing_root','./dataset/test/'),\n",
        "        ('out_dir', False),\n",
        "        ('load_dir', False)\n",
        "    ])\n",
        "    \n",
        "    batches = batches_gen(optim_config['batch_size'],train)\n",
        "\n",
        "    data_config = OrderedDict([\n",
        "        ('training_data',train),\n",
        "        ('test_data', test),\n",
        "        ('batches', batches),\n",
        "\n",
        "    ])\n",
        "\n",
        "    config = OrderedDict([\n",
        "        ('model_config', model_config),\n",
        "        ('optim_config', optim_config),\n",
        "        ('run_config', run_config),\n",
        "        ('data_config', data_config)\n",
        "    ])\n",
        "    return config"
      ],
      "metadata": {
        "id": "qIZBxL_bmSV0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    trainer = SimGNNTrainer(features, config)\n",
        "    if config['run_config']['load_dir']:\n",
        "        trainer.load()\n",
        "    else:\n",
        "        trainer.fit()\n",
        "    trainer.score()\n",
        "    if config['run_config']['out_dir']:\n",
        "        trainer.save()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = config_setup()\n",
        "    main(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT3yhAhJmWH-",
        "outputId": "95a90342-6664-4524-de5c-3e60f21e7e1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Batch Loss=0.01397): 100%|██████████| 40/40 [01:09<00:00,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 Loss:0.02600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Batch Loss=0.01397): 100%|██████████| 40/40 [01:01<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 Loss:0.02305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Batch Loss=0.01397): 100%|██████████| 40/40 [01:00<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 Loss:0.02305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Batch Loss=0.01397): 100%|██████████| 40/40 [01:02<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 Loss:0.02305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Batch Loss=0.01397): 100%|██████████| 40/40 [01:00<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 Loss:0.02305\n",
            "\n",
            "\n",
            "Model evaluation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:04<00:00, 236.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline error: 0.54748.\n",
            "\n",
            "Model test error: 0.02455.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}